{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa le librerie\n",
    "import math\n",
    "import random \n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import arange\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from math import floor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from numpy.random import seed\n",
    "#import talib\n",
    "#rom talib.abstract import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timezone, date, timedelta\n",
    "import time\n",
    "\n",
    "import tweepy\n",
    "\n",
    "import coinbasepro as cbp\n",
    "import requests\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "\n",
    "import csv\n",
    "\n",
    "import json\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" msg = tf.constant('Hello, TensorFlow!')\n",
    "tf.print(msg) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Stampa la versione di TensorFlow in uso\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bitcoin.csv', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 193 entries, 2022-12-20 to 2022-06-11\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   low              193 non-null    float64\n",
      " 1   high             193 non-null    float64\n",
      " 2   open             193 non-null    float64\n",
      " 3   close            193 non-null    float64\n",
      " 4   volume           193 non-null    float64\n",
      " 5   greed_and_fear   193 non-null    float64\n",
      " 6   difficulty       193 non-null    float64\n",
      " 7   sentiment_medio  193 non-null    float64\n",
      " 8   google_trend     193 non-null    int64  \n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 15.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>greed_and_fear</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>sentiment_medio</th>\n",
       "      <th>google_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>16398.22</td>\n",
       "      <td>17060.86</td>\n",
       "      <td>16439.98</td>\n",
       "      <td>16897.65</td>\n",
       "      <td>34330.282352</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.536407e+13</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>16273.40</td>\n",
       "      <td>16822.84</td>\n",
       "      <td>16742.33</td>\n",
       "      <td>16439.74</td>\n",
       "      <td>26856.085987</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.460660e+13</td>\n",
       "      <td>0.068184</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-18</th>\n",
       "      <td>16663.76</td>\n",
       "      <td>16875.00</td>\n",
       "      <td>16782.23</td>\n",
       "      <td>16741.16</td>\n",
       "      <td>11073.438862</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.424433e+13</td>\n",
       "      <td>0.101187</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-17</th>\n",
       "      <td>16585.64</td>\n",
       "      <td>16799.99</td>\n",
       "      <td>16634.29</td>\n",
       "      <td>16782.25</td>\n",
       "      <td>18446.683602</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.424433e+13</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>16529.53</td>\n",
       "      <td>17525.00</td>\n",
       "      <td>17359.10</td>\n",
       "      <td>16632.64</td>\n",
       "      <td>47159.863007</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.424433e+13</td>\n",
       "      <td>0.072307</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 low      high      open     close        volume  \\\n",
       "time                                                               \n",
       "2022-12-20  16398.22  17060.86  16439.98  16897.65  34330.282352   \n",
       "2022-12-19  16273.40  16822.84  16742.33  16439.74  26856.085987   \n",
       "2022-12-18  16663.76  16875.00  16782.23  16741.16  11073.438862   \n",
       "2022-12-17  16585.64  16799.99  16634.29  16782.25  18446.683602   \n",
       "2022-12-16  16529.53  17525.00  17359.10  16632.64  47159.863007   \n",
       "\n",
       "            greed_and_fear    difficulty  sentiment_medio  google_trend  \n",
       "time                                                                     \n",
       "2022-12-20            29.0  3.536407e+13         0.084011            23  \n",
       "2022-12-19            29.0  3.460660e+13         0.068184            22  \n",
       "2022-12-18            26.0  3.424433e+13         0.101187            19  \n",
       "2022-12-17            28.0  3.424433e+13         0.053202            21  \n",
       "2022-12-16            29.0  3.424433e+13         0.072307            27  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa il dataframe per ottenere gli array di input e output\n",
    "X = df.drop(columns=['close']) # tutte le colonne tranne 'close' sono gli input\n",
    "y = df['close'] # la colonna 'close' Ã¨ l'output\n",
    "\n",
    "# Divide il dataset in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardizza le features con MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model preparation and compilation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costruisci il modello di deep learning\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=184, activation='relu', input_dim=(X_train.shape[1])))\n",
    "model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "\n",
    "# Compila il modello\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra il modello sui dati di addestramento\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuta il modello utilizzando il set di test\n",
    "model.summary()\n",
    "\n",
    "predict = model.predict(X_test, verbose=0)\n",
    "\n",
    "metrics = model.evaluate(X_test, y_test, verbose=2)\n",
    "mae = mean_absolute_error(y_test, predict)\n",
    "rmse = sqrt(mean_squared_error(y_test, predict))\n",
    "print('MAE before inverse Scaling: %f' % mae)\n",
    "print('RMSE before inverse Scaling: %f' % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottieni la data di oggi\n",
    "today = datetime(2022, 12, 11).date()\n",
    "\n",
    "# Aggiungi un giorno alla data di oggi\n",
    "tomorrow = today + timedelta(days=1)\n",
    "\n",
    "# Crea un dizionario vuoto con i dati di domani\n",
    "data_tomorrow = {}\n",
    "\n",
    "# Aggiungi la data di domani al dizionario\n",
    "data_tomorrow['time'] = tomorrow\n",
    "\n",
    "# Aggiungi le altre colonne vuote al dizionario\n",
    "data_tomorrow['low'] = 0\n",
    "data_tomorrow['high'] = 0\n",
    "data_tomorrow['open'] = 0\n",
    "data_tomorrow['close'] = 0\n",
    "data_tomorrow['volume'] = 0\n",
    "data_tomorrow['greed_and_fear'] = 0\n",
    "data_tomorrow['google_trend'] = 0\n",
    "data_tomorrow['sentiment_medio'] = 0\n",
    "data_tomorrow['difficulty'] = 0\n",
    "\n",
    "# Crea un dataframe con i dati di domani\n",
    "df_tomorrow = pd.DataFrame(data_tomorrow, index=[0])\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# Unisci il dataframe di domani al dataframe storico\n",
    "df = pd.concat([df_tomorrow,df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "df = df.set_index('time')\n",
    "\n",
    "df = df.replace(0, np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['close']) # tutte le colonne tranne 'close' sono gli input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrow = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa il modello addestrato per fare previsioni sui dati di domani\n",
    "next_day_predictions = model.predict(tomorrow)\n",
    "\n",
    "print(next_day_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scarico i dati di oggi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prezzo bictoin\n",
    "client = cbp.PublicClient()\n",
    "candles = client.get_product_historic_rates(product_id=\"BTC-USD\", start=\"2022-01-11T00:00:00\", stop=\"2022-06-10T00:00:00\", granularity=86400)\n",
    "df_domani = pd.DataFrame(candles)\n",
    "\n",
    "print('dataset set 1:', df_domani.head(2))\n",
    "\n",
    "# Inizializza la lista data_list\n",
    "data_list = []\n",
    "\n",
    "# Effettua 31 chiamate all'API di Alternative.me\n",
    "for i in range(13):\n",
    "  # Prepara la richiesta HTTP\n",
    "  url = \"https://api.alternative.me/fng/?limit=13&timestamp=\" + str(i)\n",
    "  headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "  # Effettua la richiesta HTTP\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  # Carica i dati in formato JSON\n",
    "  data = response.json()\n",
    "\n",
    "  # Estrai il valore \"value\" e il timestamp\n",
    "  value = data[\"data\"][i][\"value\"]\n",
    "  timestamp = data[\"data\"][i][\"timestamp\"]\n",
    "\n",
    "  # Aggiungi i valori \"value\" e \"timestamp\" nella lista data_list\n",
    "  data_list.append((value, timestamp))\n",
    "\n",
    "  # Per ogni tupla nella lista data_list\n",
    "for value, timestamp in data_list:\n",
    "  # Converti il timestamp in una data\n",
    "  date = datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d')\n",
    "  #date = pd.to_datetime(date)\n",
    "  # Seleziona la riga del dataframe corrispondente alla data\n",
    "  df_domani.loc[df_domani['time'] == date, 'greed_and_fear'] = value\n",
    "\n",
    "print('dataset set 2:', df_domani.head(2))\n",
    "\n",
    "  # Carica il file JSON in un oggetto python\n",
    "with open('data/difficulty.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Normalizza i dati del file JSON\n",
    "df_difficulty = pd.json_normalize(data, 'difficulty', ['metric1', 'metric2'])\n",
    "\n",
    "# Crea una lista di date partendo dal 23 dicembre 2021 fino al 22 dicembre 2022\n",
    "dates = pd.date_range(start='2021-12-25', end='2022-12-23', freq='D')\n",
    "\n",
    "# Crea una nuova colonna \"date\" nel tuo dataframe e assegna ad ogni riga una data\n",
    "df_difficulty['date'] = dates\n",
    "\n",
    "df_difficulty = df_difficulty.drop(['x', 'metric1', 'metric2'], axis=1)\n",
    "\n",
    "df_difficulty.rename(columns={'date': 'time'}, inplace=True)\n",
    "\n",
    "#trasforma df['time'] in datetime\n",
    "df_difficulty['time'] = pd.to_datetime(df_difficulty['time'])\n",
    "\n",
    "#seleziono solo la data di domani\n",
    "#df_difficulty = df_difficulty.loc['2022-12-21':'2022-12-12']\n",
    "\n",
    "df_difficulty = df_difficulty.iloc[::-1]\n",
    "\n",
    "df_domani = pd.merge(df_domani, df_difficulty, on='time')\n",
    "\n",
    "df_domani.rename(columns={'y': 'difficulty'}, inplace=True)\n",
    "\n",
    "print('dataset set 3:', df_domani.head(2))\n",
    "\n",
    "#istanzio l'auth\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAPgukgEAAAAADYQXq5tFdfahfgxE%2FSQqxcV4OFU%3DIETAxHE05aQI4saJCxFJMQiaurCSZgYMSHZ9kl7SMVNNxM5V4S\"  # BEARER_TOKEN\n",
    "auth = tweepy.Client(bearer_token)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#scarico tweets di oggi\n",
    "\n",
    "# Numero di giorni da scaricare\n",
    "num_days = 13\n",
    "\n",
    "# Numero di tweet da scaricare ogni giorno\n",
    "num_tweets = 500\n",
    "\n",
    "# Inizializza una lista che conterrÃ  i tweet scaricati\n",
    "tweet_list = []\n",
    "\n",
    "# Inizializza la data di inizio del loop\n",
    "start_date = today = datetime.today() - timedelta(days=14)\n",
    "#start_date = datetime.now() - timedelta(days=30)\n",
    "\n",
    "# Crea il loop\n",
    "for i in range(num_days):\n",
    "    # Aggiorna la data di fine\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "\n",
    "    # Formatta le date come stringhe nel formato richiesto dall'API\n",
    "    formatted_start_date = start_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    formatted_end_date = end_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    #hashtag\n",
    "    query= \"bitcoin\"\n",
    "\n",
    "    # Scarica i tweet\n",
    "    response = auth.search_all_tweets(query, max_results=num_tweets, tweet_fields=[\"created_at\", \"lang\"], start_time=start_date, end_time=end_date)\n",
    "    tweets = response.data\n",
    "\n",
    "    # Aggiungi i tweet alla lista\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append([tweet.created_at, tweet.id, tweet.text, tweet.lang])\n",
    "\n",
    "    # Aggiorna la data di inizio\n",
    "    start_date = end_date\n",
    "    time.sleep(5)\n",
    "\n",
    "df_sentiment = pd.DataFrame(data=tweet_list, columns=['created_at', 'tweet_id', 'text', 'language'])  \n",
    "df_sentiment.set_index(\"created_at\", inplace = True)\n",
    "\n",
    "df_sentiment = df_sentiment.iloc[::-1]\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Rimuove i caratteri di nuova riga e a capo\n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # Rimuove i link\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # Rimuove gli hashtag\n",
    "    tweet = re.sub(r\"#\\S+\", \"\", tweet)\n",
    "    \n",
    "    # Rimuove le menzioni\n",
    "    tweet = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    \n",
    "    # Rimuove i caratteri di punteggiatura\n",
    "    tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Converti il testo in minuscolo\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "# Crea una copia della colonna 'text' del dataframe\n",
    "df_sentiment['text_clean'] = df_sentiment['text'].copy()\n",
    "\n",
    "# Sostituisci i testi elaborati nella colonna 'text_clean' del dataframe\n",
    "df_sentiment['text_clean'] = df_sentiment['text_clean'].apply(preprocess_tweet)\n",
    "\n",
    "def get_sentiment(tweet_text):\n",
    "    analysis = TextBlob(tweet_text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "df_sentiment['sentiment'] = df_sentiment['text_clean'].apply(get_sentiment)\n",
    "\n",
    "df_sentiment = df_sentiment.reset_index()\n",
    "df_sentiment['created_at'] = pd.to_datetime(df_sentiment['created_at'])\n",
    "\n",
    "# Crea una nuova colonna 'date' che estrae la data dalla colonna 'created_at'\n",
    "df_sentiment['date'] = df_sentiment['created_at'].dt.date\n",
    "\n",
    "# raggruppa i dati per data utilizzando la colonna 'data' come chiave di raggruppamento\n",
    "grouped_data = df_sentiment.groupby('date')\n",
    "\n",
    "# per ogni gruppo di dati, calcola la mediana del sentiment e aggiungi il valore alla lista medie\n",
    "medie = []\n",
    "for name, group in grouped_data:\n",
    "    medie.append(mean(group['sentiment']))\n",
    "\n",
    "# crea il dizionario con le date come chiavi e i valori medi come valori\n",
    "dati = {key: value for key, value in zip(grouped_data.groups.keys(), medie)}\n",
    "\n",
    "# crea il DataFrame utilizzando il dizionario appena creato\n",
    "df_medie = pd.DataFrame.from_dict(dati, orient='index')\n",
    "\n",
    "# rinomina la colonna del DataFrame con il nome desiderato\n",
    "df_medie.rename(columns={0: 'sentiment_medio'}, inplace=True)\n",
    "\n",
    "df_medie = df_medie.reset_index()\n",
    "\n",
    "df_medie.rename(columns={'index': 'time'}, inplace=True)\n",
    "\n",
    "df_medie = df_medie.iloc[::-1]\n",
    "\n",
    "#trasformo la colonna created_at in datetime\n",
    "df_medie['time'] = pd.to_datetime(df_medie['time'])\n",
    "\n",
    "#trasforma df['time'] in datetime\n",
    "df_domani['time'] = pd.to_datetime(df_domani['time'])\n",
    "\n",
    "# Unisci i due dataframe utilizzando la colonna \"time\" come chiave di unione\n",
    "df_domani = pd.merge(df_domani, df_medie, on='time')\n",
    "\n",
    "print('dataset set 4:', df_domani.head(2))\n",
    "\n",
    "#scarico google trends\n",
    "df_google = pd.read_csv('data/multiTimeline.csv')\n",
    "df_google['time'] = pd.to_datetime(df_google['time'])\n",
    "df_domani = pd.merge(df_domani, df_google, on='time')\n",
    "df_domani.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unisco i nuovi dati al database originale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df = df.reset_index()\n",
    "df_domani = df_domani.reset_index()\n",
    "\n",
    "# Unisci il dataframe di domani al dataframe storico\n",
    "df = pd.concat([df_domani,df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "df = df.set_index('time') \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('time')\n",
    "df.to_csv('data/bitcoin.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparo i nuovi dati per la previsione**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Usa il modello addestrato per fare previsioni sui dati di domani\n",
    "next_day_predictions = model.predict(domani)\n",
    "\n",
    "print(next_day_predictions) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95fe2f621b02c291269f6f1de04399c71d5624912c164c733c30837729478179"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
